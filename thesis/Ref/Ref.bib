% Encoding: UTF-8

@InProceedings{5377665,
  author    = {Kapre, Nachiket and DeHon, André},
  booktitle = {2009 International Conference on Field-Programmable Technology},
  date      = {2009-12},
  title     = {{Parallelizing sparse Matrix Solve for SPICE circuit simulation using FPGAs}},
  doi       = {10.1109/FPT.2009.5377665},
  pages     = {190-198},
  abstract  = {Fine-grained dataflow processing of sparse matrix-solve computation (Ax¿ = b¿) in the SPICE circuit simulator can provide an order of magnitude performance improvement on modern FPGAs. Matrix solve is the dominant component of the simulator especially for large circuits and is invoked repeatedly during the simulation, once for every iteration. We process sparse-matrix computation generated from the SPICE-oriented KLU solver in dataflow fashion across multiple spatial floating-point operators coupled to high-bandwidth on-chip memories and interconnected by a low-latency network. Using this approach, we are able to show speedups of 1.2-64× (geometric mean of 8.8×) for a range of circuits and benchmark matrices when comparing double-precision implementations on a 250 MHz Xilinx Virtex-5 FPGA (65 nm) and an Intel Core i7 965 processor (45 nm).},
  file      = {:Parallelizing_sparse_Matrix_Solve_for_SPICE_circuit_simulation_using_FPGAs.pdf:PDF},
}

@InProceedings{6241646,
  author    = {Ren, Ling and Chen, Xiaoming and Wang, Yu and Zhang, Chenxi and Yang, Huazhong},
  booktitle = {DAC Design Automation Conference 2012},
  date      = {2012-06},
  title     = {{Sparse LU factorization for parallel circuit simulation on GPU}},
  pages     = {1125-1130},
  url       = {https://ieeexplore.ieee.org/document/6241646},
  abstract  = {Sparse solver has become the bottleneck of SPICE simulators. There has been few work on GPU-based sparse solver because of the high data-dependency. The strong data-dependency determines that parallel sparse LU factorization runs efficiently on shared-memory computing devices. But the number of CPU cores sharing the same memory is often limited. The state of the art Graphic Processing Units (GPU) naturally have numerous cores sharing the device memory, and provide a possible solution to the problem. In this paper, we propose a GPU-based sparse LU solver for circuit simulation. We optimize the work partitioning, the number of active thread groups, and the memory access pattern, based on GPU architecture. On matrices whose factorization involves many floating-point operations, our GPU-based sparse LU factorization achieves 7.90× speedup over 1-core CPU and 1.49× speedup over 8-core CPU. We also analyze the scalability of parallel sparse LU factorization and investigate the specifications on CPUs and GPUs that most influence the performance.},
  file      = {:Sparse_LU_factorization_for_parallel_circuit_simulation_on_GPU.pdf:PDF},
  issn      = {0738-100X},
}

@Article{6774937,
  author       = {Chen, Xiaoming and Ren, Ling and Wang, Yu and Yang, Huazhong},
  date         = {2015-03},
  journaltitle = {IEEE Transactions on Parallel and Distributed Systems},
  title        = {{GPU-Accelerated Sparse LU Factorization for Circuit Simulation with Performance Modeling}},
  doi          = {10.1109/TPDS.2014.2312199},
  issn         = {1558-2183},
  number       = {3},
  pages        = {786-795},
  volume       = {26},
  abstract     = {The sparse matrix solver by LU factorization is a serious bottleneck in Simulation Program with Integrated Circuit Emphasis (SPICE)-based circuit simulators. The state-of-the-art Graphics Processing Units (GPU) have numerous cores sharing the same memory, provide attractive memory bandwidth and compute capability, and support massive thread-level parallelism, so GPUs can potentially accelerate the sparse solver in circuit simulators. In this paper, an efficient GPU-based sparse solver for circuit problems is proposed. We develop a hybrid parallel LU factorization approach combining task-level and data-level parallelism on GPUs. Work partitioning, number of active thread groups, and memory access patterns are optimized based on the GPU architecture. Experiments show that the proposed LU factorization approach on NVIDIA GTX580 attains an average speedup of 7.02× (geometric mean) compared with sequential PARDISO, and 1.55× compared with 16-threaded PARDISO. We also investigate bottlenecks of the proposed approach by a parametric performance model. The performance of the sparse LU factorization on GPUs is constrained by the global memory bandwidth, so the performance can be further improved by future GPUs with larger memory bandwidth.},
  file         = {:GPU-Accelerated_Sparse_LU_Factorization_for_Circuit_Simulation_with_Performance_Modeling.pdf:PDF},
}

@InProceedings{6164974,
  author    = {Chen, Xiaoming and Wang, Yu and Yang, Huazhong},
  booktitle = {17th Asia and South Pacific Design Automation Conference},
  date      = {2012-01},
  title     = {{An adaptive LU factorization algorithm for parallel circuit simulation}},
  doi       = {10.1109/ASPDAC.2012.6164974},
  pages     = {359-364},
  abstract  = {Sparse matrix solver has become the bottleneck in SPICE simulator. It is difficult to parallelize the solver because of the high data-dependency during the numerical LU factorization. This paper proposes a parallel LU factorization (with partial pivoting) algorithm on shared-memory computers with multi-core CPUs, to accelerate circuit simulation. Since not every matrix is suitable for parallel algorithm, a predictive method is proposed to decide whether a matrix should use parallel or sequential algorithm. The experimental results on 35 circuit matrices reveal that the developed algorithm achieves speedups of 2.11×~8.38× (on geometric-average), compared with KLU, with 1~8 threads, on the matrices which are suitable for parallel algorithm. Our solver can be downloaded from http://nicslu.weebly.com.},
  file      = {:An_adaptive_LU_factorization_algorithm_for_parallel_circuit_simulation.pdf:PDF},
  issn      = {2153-697X},
}

@Article{8430608,
  author       = {Lee, Wai-Kong and Achar, Ramachandra and Nakhla, Michel S.},
  date         = {2018-11},
  journaltitle = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  title        = {{Dynamic GPU Parallel Sparse LU Factorization for Fast Circuit Simulation}},
  doi          = {10.1109/TVLSI.2018.2858014},
  issn         = {1557-9999},
  number       = {11},
  pages        = {2518-2529},
  volume       = {26},
  abstract     = {Lower-upper (LU) factorization is widely used in many scientific computations. It is one of the most critical modules in circuit simulators, such as the Simulation Program With Integrated Circuit Emphasis. To exploit the emerging graphics process unit (GPU) computing platforms, several GPU-based sparse LU solvers have been recently proposed. In this paper, efficient algorithms are presented to enhance the ability of GPU-based LU solvers to achieve higher parallelism as well as to exploit the dynamic parallelism feature in the state-of-the-art GPUs. Also, rigorous performance comparisons of the proposed algorithms with GLU as well as KLU, for both the single-precision and double-precision cases, are presented.},
  file         = {:Dynamic_GPU_Parallel_Sparse_LU_Factorization_for_Fast_Circuit_Simulation.pdf:PDF},
}

@InProceedings{9105410,
  author    = {Mahajan, Yogesh and Obla, Shashank and Namboothiripad, Mini K. and Datar, Mandar J. and Sharma, Niraj N. and Patkar, Sachin B.},
  booktitle = {2020 33rd International Conference on VLSI Design and 2020 19th International Conference on Embedded Systems (VLSID)},
  date      = {2020-01},
  title     = {{FPGA-Based Acceleration of LU decomposition for Analog and RF Circuit Simulation}},
  doi       = {10.1109/VLSID49098.2020.00040},
  pages     = {131-136},
  abstract  = {It is well known that solving a sparse system of linear equations is the workhorse critical step in analog and RF circuit simulation. The LU factorization of sparse system matrix is a key operation in order to solve a system of linear equations. Extracting parallelism to factorize a sparse matrix requires knowledge of pattern of non-zeros in the matrix. Exploiting the fact that sparsity patterns of matrices involved in circuit simulations do not change across iterations, the schedule of computing steps of LU factorization can be statically determined and analyzed for parallelism. This approach has been well investigated by [1] and [2] in recent years. Our work improvises this well-researched approach and exposes more parallelism, thereby yielding improved results (by factor up to 2). The accelerated design is prototyped using custom high-level-synthesis tool specific to LU decomposition problem. The case of RF Circuits is treated slightly differently. Such circuits have a number of nonlinear components. Periodic steady state response can be determined efficiently using Harmonic Balance method even when amplitudes are not small. Acceleration of this method has been a subject of vigorous interest in recent years [4], [7]. The system matrix appearing in this method is a sparse set of tiles whose solution can benefit from the techniques mentioned above. The sparsity pattern is inherited from the circuit connections. This paper describes FPGA-and manycore-based acceleration of automated Harmonic Balance (HB) simulator which simulates a circuit with a netlist similar to SPICE. Our 100MHz custom digital design on the FPGA of ZedBoard (XC7Z020) yields a speed up of up to 4.39 over the on-chip dual Core ARM Cortex-A9, 666 MHz processor. And on Intel's Xeon Phi Knights Landing (KNL) manycore chip, we obtain a speed up of up to 8.96 over a 4-core Intel i7 processor running at 3GHz clock frequency.},
  file      = {:FPGA-Based_Acceleration_of_LU_decomposition_for_Analog_and_RF_Circuit_Simulation.pdf:PDF},
  issn      = {2380-6923},
}

@PhdThesis{natarajan2005klu,
  author      = {Natarajan, Ekanathan Palamadai},
  date        = {2005},
  institution = {University of Florida},
  title       = {{KLU--A high performance sparse linear solver for circuit simulation problems}},
  url         = {https://ufdc.ufl.edu/UFE0011721/00001/citation},
  file        = {:KLU--A high performance sparse linear solver for circuit simulation problems.pdf:PDF},
}

@Article{doi:10.1137/0909058,
  author       = {Gilbert, John R. and Peierls, Tim},
  date         = {1988},
  journaltitle = {SIAM Journal on Scientific and Statistical Computing},
  title        = {{Sparse Partial Pivoting in Time Proportional to Arithmetic Operations}},
  doi          = {10.1137/0909058},
  eprint       = {https://doi.org/10.1137/0909058},
  number       = {5},
  pages        = {862-874},
  url          = {https://doi.org/10.1137/0909058},
  volume       = {9},
  file         = {:Sparse Partial Pivoting in Time Proportional to Arithmetic Operations.pdf:PDF},
}

@Article{5710875,
  author       = {Jaiswal, Manish Kumar and Chandrachoodan, Nitin},
  date         = {2012-01},
  journaltitle = {IEEE Transactions on Computers},
  title        = {{FPGA-Based High-Performance and Scalable Block LU Decomposition Architecture}},
  doi          = {10.1109/TC.2011.24},
  issn         = {1557-9956},
  number       = {1},
  pages        = {60-72},
  volume       = {61},
  abstract     = {Decomposition of a matrix into lower and upper triangular matrices (LU decomposition) is a vital part of many scientific and engineering applications, and the block LU decomposition algorithm is an approach well suited to parallel hardware implementation. This paper presents an approach to speed up implementation of the block LU decomposition algorithm using FPGA hardware. Unlike most previous approaches reported in the literature, the approach does not assume the matrix can be stored entirely on chip. The memory accesses are studied for various FPGA configurations, and a schedule of operations for scaling well is shown. The design has been synthesized for FPGA targets and can be easily retargeted. The design outperforms previous hardware implementations, as well as tuned software implementations including the ATLAS and MKL libraries on workstations.},
  file         = {:FPGA-Based_High-Performance_and_Scalable_Block_LU_Decomposition_Architecture.pdf:PDF},
}

@Article{10.1145/1824801.1824814,
  author       = {Davis, Timothy A. and Palamadai Natarajan, Ekanathan},
  date         = {2010-09},
  journaltitle = {ACM Trans. Math. Softw.},
  title        = {{Algorithm 907: Klu, a Direct Sparse Solver for Circuit Simulation Problems}},
  doi          = {10.1145/1824801.1824814},
  issn         = {0098-3500},
  number       = {3},
  url          = {https://doi.org/10.1145/1824801.1824814},
  volume       = {37},
  abstract     = {KLU is a software package for solving sparse unsymmetric linear systems of equations that arise in circuit simulation applications. It relies on a permutation to Block Triangular Form (BTF), several methods for finding a fill-reducing ordering (variants of approximate minimum degree and nested dissection), and Gilbert/Peierls’ sparse left-looking LU factorization algorithm to factorize each block. The package is written in C and includes a MATLAB interface. Performance results comparing KLU with SuperLU, Sparse 1.3, and UMFPACK on circuit simulation matrices are presented. KLU is the default sparse direct solver in the XyceTMcircuit simulation package developed by Sandia National Laboratories.},
  articleno    = {36},
  file         = {:KLU, A Direct Sparse Solver for Circuit Simulation Problems.pdf:PDF},
  issue_date   = {September 2010},
  keywords     = {LU factorization, circuit simulation, sparse matrices},
  location     = {New York, NY, USA},
  numpages     = {17},
  publisher    = {Association for Computing Machinery},
}

@Article{DBLP:journals/corr/abs-1805-08288,
  author       = {Johannes de Fine Licht and Maciej Besta and Simon Meierhans and Torsten Hoefler},
  date         = {2018},
  journaltitle = {CoRR},
  title        = {{Transformations of High-Level Synthesis Codes for High-Performance Computing}},
  eprint       = {1805.08288},
  eprinttype   = {arxiv},
  volume       = {abs/1805.08288},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1805-08288.bib},
  file         = {:Transformations of High-Level Synthesis Codes for High-Performance Computing.pdf:PDF},
  timestamp    = {Wed, 16 Jun 2021 11:47:25 +0200},
}

@Book{10.5555/2685817,
  author    = {Crockett, Louise H. and Elliot, Ross A. and Enderwitz, Martin A. and Stewart, Robert W.},
  date      = {2014},
  title     = {{The Zynq Book: Embedded Processing with the Arm Cortex-A9 on the Xilinx Zynq-7000 All Programmable Soc}},
  isbn      = {099297870X},
  location  = {Glasgow, GBR},
  publisher = {Strathclyde Academic Media},
  url       = {http://www.zynqbook.com/},
  abstract  = {This book is about the Zynq-7000 All Programmable System on Chip, the family of devices from Xilinx that combines an application-grade ARM Cortex-A9 processor with traditional FPGA logic fabric. Catering for both new and experienced readers, it covers fundamental issues in an accessible way, starting with a clear overview of the device architecture, and an introduction to the design tools and processes for developing a Zynq SoC. Later chapters progress to more advanced topics such as embedded systems development, IP block design and operating systems. Maintaining a 'real-world' perspective, the book also compares Zynq with other device alternatives, and considers end-user applications. The Zynq Book is accompanied by a set of practical tutorials hosted on a companion website. These tutorials will guide the reader through first steps with Zynq, following on to a complete, audio-based embedded systems design.},
  file      = {:D\:/74133/OneDrive - University of Edinburgh/Senior/Project/Vitis/The_Zynq_Book_ebook_chinese.pdf:PDF},
}

@Book{doi:10.1137/1.9780898718881,
  author    = {Davis, Timothy A.},
  date      = {2006},
  title     = {{Direct Methods for Sparse Linear Systems}},
  doi       = {10.1137/1.9780898718881},
  eprint    = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898718881},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://epubs.siam.org/doi/abs/10.1137/1.9780898718881},
  file      = {:Direct Methods for Sparse Linear Systems by Timothy A. Davis.pdf:PDF},
}

@Article{doi:10.1137/S0895479895291765,
  author       = {Demmel, James W. and Eisenstat, Stanley C. and Gilbert, John R. and Li, Xiaoye S. and Liu, Joseph W. H.},
  date         = {1999},
  journaltitle = {SIAM Journal on Matrix Analysis and Applications},
  title        = {{A Supernodal Approach to Sparse Partial Pivoting}},
  doi          = {10.1137/S0895479895291765},
  eprint       = {https://doi.org/10.1137/S0895479895291765},
  number       = {3},
  pages        = {720-755},
  url          = {https://doi.org/10.1137/S0895479895291765},
  volume       = {20},
  file         = {:A Supernodal Approach to Sparse Partial Pivoting.pdf:PDF},
}

@Article{article,
  author = {Boisvert, Ronald and Pozo, Roldan and Remington, Karin},
  date   = {1997-03},
  title  = {{The Matrix Market Exchange Formats: Initial Design}},
  url    = {https://www.researchgate.net/publication/2630533_The_Matrix_Market_Exchange_Formats_Initial_Design},
  file   = {:The Matrix Market Exchange Formats - Initial Design.pdf:PDF},
}

@Article{2018arXiv180503648K,
  author       = {{Kastner}, Ryan and {Matai}, Janarbek and {Neuendorffer}, Stephen},
  date         = {2018-05},
  journaltitle = {arXiv e-prints},
  title        = {{Parallel Programming for FPGAs}},
  eid          = {arXiv:1805.03648},
  eprint       = {1805.03648},
  eprintclass  = {cs.AR},
  eprinttype   = {arXiv},
  pages        = {arXiv:1805.03648},
  adsnote      = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl       = {https://ui.adsabs.harvard.edu/abs/2018arXiv180503648K},
  file         = {:D\:/74133/OneDrive - University of Edinburgh/Senior/Project/Vitis/Parallel Programming for FPGAs.pdf:PDF},
  keywords     = {Computer Science - Hardware Architecture},
}

@Article{10.1145/1024074.1024081,
  author     = {Amestoy, Patrick R. and Davis, Timothy A. and Duff, Iain S.},
  title      = {Algorithm 837: AMD, an Approximate Minimum Degree Ordering Algorithm},
  doi        = {10.1145/1024074.1024081},
  issn       = {0098-3500},
  number     = {3},
  pages      = {381–388},
  url        = {https://doi.org/10.1145/1024074.1024081},
  volume     = {30},
  abstract   = {AMD is a set of routines that implements the approximate minimum degree ordering algorithm to permute sparse matrices prior to numerical factorization. There are versions written in both C and Fortran 77. A MATLAB interface is included.},
  address    = {New York, NY, USA},
  file       = {:AMD, an approximate minimum degree ordering algorithm.pdf:PDF},
  issue_date = {September 2004},
  journal    = {ACM Trans. Math. Softw.},
  keywords   = {sparse matrices, Linear equations, minimum degree, ordering methods},
  month      = {sep},
  numpages   = {8},
  publisher  = {Association for Computing Machinery},
  year       = {2004},
}

@Comment{jabref-meta: databaseType:biblatex;}

@Comment{jabref-meta: fileDirectoryLatex-74133-YCZhang:D:\\74133\\OneDrive - University of Edinburgh\\Senior\\Project\\thesis;}
