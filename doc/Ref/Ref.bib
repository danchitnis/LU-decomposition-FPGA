@InProceedings{Kapre2009,
  author    = {Kapre, Nachiket and DeHon, André},
  booktitle = {2009 International Conference on Field-Programmable Technology},
  date      = {2009-12},
  title     = {{Parallelizing sparse Matrix Solve for SPICE circuit simulation using FPGAs}},
  doi       = {10.1109/FPT.2009.5377665},
  pages     = {190-198},
  abstract  = {Fine-grained dataflow processing of sparse matrix-solve computation (Ax¿ = b¿) in the SPICE circuit simulator can provide an order of magnitude performance improvement on modern FPGAs. Matrix solve is the dominant component of the simulator especially for large circuits and is invoked repeatedly during the simulation, once for every iteration. We process sparse-matrix computation generated from the SPICE-oriented KLU solver in dataflow fashion across multiple spatial floating-point operators coupled to high-bandwidth on-chip memories and interconnected by a low-latency network. Using this approach, we are able to show speedups of 1.2-64× (geometric mean of 8.8×) for a range of circuits and benchmark matrices when comparing double-precision implementations on a 250 MHz Xilinx Virtex-5 FPGA (65 nm) and an Intel Core i7 965 processor (45 nm).},
  file      = {:Parallelizing_sparse_Matrix_Solve_for_SPICE_circuit_simulation_using_FPGAs.pdf:PDF},
  groups    = {FPGA},
}

@InProceedings{Ren2012,
  author    = {Ren, Ling and Chen, Xiaoming and Wang, Yu and Zhang, Chenxi and Yang, Huazhong},
  booktitle = {DAC Design Automation Conference 2012},
  date      = {2012-06},
  title     = {{Sparse LU factorization for parallel circuit simulation on GPU}},
  pages     = {1125-1130},
  url       = {https://ieeexplore.ieee.org/document/6241646},
  abstract  = {Sparse solver has become the bottleneck of SPICE simulators. There has been few work on GPU-based sparse solver because of the high data-dependency. The strong data-dependency determines that parallel sparse LU factorization runs efficiently on shared-memory computing devices. But the number of CPU cores sharing the same memory is often limited. The state of the art Graphic Processing Units (GPU) naturally have numerous cores sharing the device memory, and provide a possible solution to the problem. In this paper, we propose a GPU-based sparse LU solver for circuit simulation. We optimize the work partitioning, the number of active thread groups, and the memory access pattern, based on GPU architecture. On matrices whose factorization involves many floating-point operations, our GPU-based sparse LU factorization achieves 7.90× speedup over 1-core CPU and 1.49× speedup over 8-core CPU. We also analyze the scalability of parallel sparse LU factorization and investigate the specifications on CPUs and GPUs that most influence the performance.},
  file      = {:Sparse_LU_factorization_for_parallel_circuit_simulation_on_GPU.pdf:PDF},
  groups    = {GPU},
  issn      = {0738-100X},
}

@Article{Chen2015,
  author       = {Chen, Xiaoming and Ren, Ling and Wang, Yu and Yang, Huazhong},
  date         = {2015-03},
  journaltitle = {IEEE Transactions on Parallel and Distributed Systems},
  title        = {{GPU-Accelerated Sparse LU Factorization for Circuit Simulation with Performance Modeling}},
  doi          = {10.1109/TPDS.2014.2312199},
  issn         = {1558-2183},
  number       = {3},
  pages        = {786-795},
  volume       = {26},
  abstract     = {The sparse matrix solver by LU factorization is a serious bottleneck in Simulation Program with Integrated Circuit Emphasis (SPICE)-based circuit simulators. The state-of-the-art Graphics Processing Units (GPU) have numerous cores sharing the same memory, provide attractive memory bandwidth and compute capability, and support massive thread-level parallelism, so GPUs can potentially accelerate the sparse solver in circuit simulators. In this paper, an efficient GPU-based sparse solver for circuit problems is proposed. We develop a hybrid parallel LU factorization approach combining task-level and data-level parallelism on GPUs. Work partitioning, number of active thread groups, and memory access patterns are optimized based on the GPU architecture. Experiments show that the proposed LU factorization approach on NVIDIA GTX580 attains an average speedup of 7.02× (geometric mean) compared with sequential PARDISO, and 1.55× compared with 16-threaded PARDISO. We also investigate bottlenecks of the proposed approach by a parametric performance model. The performance of the sparse LU factorization on GPUs is constrained by the global memory bandwidth, so the performance can be further improved by future GPUs with larger memory bandwidth.},
  file         = {:GPU-Accelerated_Sparse_LU_Factorization_for_Circuit_Simulation_with_Performance_Modeling.pdf:PDF},
  groups       = {GPU},
}

@InProceedings{Chen2012,
  author    = {Chen, Xiaoming and Wang, Yu and Yang, Huazhong},
  booktitle = {17th Asia and South Pacific Design Automation Conference},
  date      = {2012-01},
  title     = {{An adaptive LU factorization algorithm for parallel circuit simulation}},
  doi       = {10.1109/ASPDAC.2012.6164974},
  pages     = {359-364},
  abstract  = {Sparse matrix solver has become the bottleneck in SPICE simulator. It is difficult to parallelize the solver because of the high data-dependency during the numerical LU factorization. This paper proposes a parallel LU factorization (with partial pivoting) algorithm on shared-memory computers with multi-core CPUs, to accelerate circuit simulation. Since not every matrix is suitable for parallel algorithm, a predictive method is proposed to decide whether a matrix should use parallel or sequential algorithm. The experimental results on 35 circuit matrices reveal that the developed algorithm achieves speedups of 2.11×~8.38× (on geometric-average), compared with KLU, with 1~8 threads, on the matrices which are suitable for parallel algorithm. Our solver can be downloaded from http://nicslu.weebly.com.},
  file      = {:An_adaptive_LU_factorization_algorithm_for_parallel_circuit_simulation.pdf:PDF},
  groups    = {Algorithm},
  issn      = {2153-697X},
}

@Article{Lee2018,
  author       = {Lee, Wai-Kong and Achar, Ramachandra and Nakhla, Michel S.},
  date         = {2018-11},
  journaltitle = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  title        = {{Dynamic GPU Parallel Sparse LU Factorization for Fast Circuit Simulation}},
  doi          = {10.1109/TVLSI.2018.2858014},
  issn         = {1557-9999},
  number       = {11},
  pages        = {2518-2529},
  volume       = {26},
  abstract     = {Lower-upper (LU) factorization is widely used in many scientific computations. It is one of the most critical modules in circuit simulators, such as the Simulation Program With Integrated Circuit Emphasis. To exploit the emerging graphics process unit (GPU) computing platforms, several GPU-based sparse LU solvers have been recently proposed. In this paper, efficient algorithms are presented to enhance the ability of GPU-based LU solvers to achieve higher parallelism as well as to exploit the dynamic parallelism feature in the state-of-the-art GPUs. Also, rigorous performance comparisons of the proposed algorithms with GLU as well as KLU, for both the single-precision and double-precision cases, are presented.},
  file         = {:Dynamic_GPU_Parallel_Sparse_LU_Factorization_for_Fast_Circuit_Simulation.pdf:PDF},
  groups       = {GPU},
}

@InProceedings{Mahajan2020,
  author    = {Mahajan, Yogesh and Obla, Shashank and Namboothiripad, Mini K. and Datar, Mandar J. and Sharma, Niraj N. and Patkar, Sachin B.},
  booktitle = {2020 33rd International Conference on VLSI Design and 2020 19th International Conference on Embedded Systems (VLSID)},
  date      = {2020-01},
  title     = {{FPGA-Based Acceleration of LU decomposition for Analog and RF Circuit Simulation}},
  doi       = {10.1109/VLSID49098.2020.00040},
  pages     = {131-136},
  abstract  = {It is well known that solving a sparse system of linear equations is the workhorse critical step in analog and RF circuit simulation. The LU factorization of sparse system matrix is a key operation in order to solve a system of linear equations. Extracting parallelism to factorize a sparse matrix requires knowledge of pattern of non-zeros in the matrix. Exploiting the fact that sparsity patterns of matrices involved in circuit simulations do not change across iterations, the schedule of computing steps of LU factorization can be statically determined and analyzed for parallelism. This approach has been well investigated by [1] and [2] in recent years. Our work improvises this well-researched approach and exposes more parallelism, thereby yielding improved results (by factor up to 2). The accelerated design is prototyped using custom high-level-synthesis tool specific to LU decomposition problem. The case of RF Circuits is treated slightly differently. Such circuits have a number of nonlinear components. Periodic steady state response can be determined efficiently using Harmonic Balance method even when amplitudes are not small. Acceleration of this method has been a subject of vigorous interest in recent years [4], [7]. The system matrix appearing in this method is a sparse set of tiles whose solution can benefit from the techniques mentioned above. The sparsity pattern is inherited from the circuit connections. This paper describes FPGA-and manycore-based acceleration of automated Harmonic Balance (HB) simulator which simulates a circuit with a netlist similar to SPICE. Our 100MHz custom digital design on the FPGA of ZedBoard (XC7Z020) yields a speed up of up to 4.39 over the on-chip dual Core ARM Cortex-A9, 666 MHz processor. And on Intel's Xeon Phi Knights Landing (KNL) manycore chip, we obtain a speed up of up to 8.96 over a 4-core Intel i7 processor running at 3GHz clock frequency.},
  file      = {:FPGA-Based_Acceleration_of_LU_decomposition_for_Analog_and_RF_Circuit_Simulation.pdf:PDF},
  groups    = {FPGA},
  issn      = {2380-6923},
}

@PhdThesis{Natarajan2005,
  author      = {Natarajan, Ekanathan Palamadai},
  date        = {2005},
  institution = {University of Florida},
  title       = {{KLU--A high performance sparse linear solver for circuit simulation problems}},
  url         = {https://ufdc.ufl.edu/UFE0011721/00001/citation},
  file        = {:KLU--A high performance sparse linear solver for circuit simulation problems.pdf:PDF},
  groups      = {Algorithm},
}

@Article{Gilbert1988,
  author       = {Gilbert, John R. and Peierls, Tim},
  date         = {1988},
  journaltitle = {SIAM Journal on Scientific and Statistical Computing},
  title        = {{Sparse Partial Pivoting in Time Proportional to Arithmetic Operations}},
  doi          = {10.1137/0909058},
  eprint       = {https://doi.org/10.1137/0909058},
  number       = {5},
  pages        = {862-874},
  url          = {https://doi.org/10.1137/0909058},
  volume       = {9},
  file         = {:Sparse Partial Pivoting in Time Proportional to Arithmetic Operations.pdf:PDF},
  groups       = {Algorithm},
}

@Article{Jaiswal2012,
  author       = {Jaiswal, Manish Kumar and Chandrachoodan, Nitin},
  date         = {2012-01},
  journaltitle = {IEEE Transactions on Computers},
  title        = {{FPGA-Based High-Performance and Scalable Block LU Decomposition Architecture}},
  doi          = {10.1109/TC.2011.24},
  issn         = {1557-9956},
  number       = {1},
  pages        = {60-72},
  volume       = {61},
  abstract     = {Decomposition of a matrix into lower and upper triangular matrices (LU decomposition) is a vital part of many scientific and engineering applications, and the block LU decomposition algorithm is an approach well suited to parallel hardware implementation. This paper presents an approach to speed up implementation of the block LU decomposition algorithm using FPGA hardware. Unlike most previous approaches reported in the literature, the approach does not assume the matrix can be stored entirely on chip. The memory accesses are studied for various FPGA configurations, and a schedule of operations for scaling well is shown. The design has been synthesized for FPGA targets and can be easily retargeted. The design outperforms previous hardware implementations, as well as tuned software implementations including the ATLAS and MKL libraries on workstations.},
  file         = {:FPGA-Based_High-Performance_and_Scalable_Block_LU_Decomposition_Architecture.pdf:PDF},
  groups       = {FPGA},
}

@Article{Davis2010,
  author       = {Davis, Timothy A. and Palamadai Natarajan, Ekanathan},
  date         = {2010-09},
  journaltitle = {ACM Trans. Math. Softw.},
  title        = {{Algorithm 907: Klu, a Direct Sparse Solver for Circuit Simulation Problems}},
  doi          = {10.1145/1824801.1824814},
  issn         = {0098-3500},
  number       = {3},
  url          = {https://doi.org/10.1145/1824801.1824814},
  volume       = {37},
  abstract     = {KLU is a software package for solving sparse unsymmetric linear systems of equations that arise in circuit simulation applications. It relies on a permutation to Block Triangular Form (BTF), several methods for finding a fill-reducing ordering (variants of approximate minimum degree and nested dissection), and Gilbert/Peierls’ sparse left-looking LU factorization algorithm to factorize each block. The package is written in C and includes a MATLAB interface. Performance results comparing KLU with SuperLU, Sparse 1.3, and UMFPACK on circuit simulation matrices are presented. KLU is the default sparse direct solver in the XyceTMcircuit simulation package developed by Sandia National Laboratories.},
  articleno    = {36},
  file         = {:KLU, A Direct Sparse Solver for Circuit Simulation Problems.pdf:PDF},
  groups       = {Algorithm},
  issue_date   = {September 2010},
  keywords     = {LU factorization, circuit simulation, sparse matrices},
  location     = {New York, NY, USA},
  numpages     = {17},
  publisher    = {Association for Computing Machinery},
}

@Article{FineLicht2018,
  author       = {Johannes de Fine Licht and Maciej Besta and Simon Meierhans and Torsten Hoefler},
  date         = {2018},
  journaltitle = {CoRR},
  title        = {{Transformations of High-Level Synthesis Codes for High-Performance Computing}},
  eprint       = {1805.08288},
  eprinttype   = {arxiv},
  volume       = {abs/1805.08288},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1805-08288.bib},
  file         = {:Transformations of High-Level Synthesis Codes for High-Performance Computing.pdf:PDF},
  groups       = {Algorithm},
  timestamp    = {Wed, 16 Jun 2021 11:47:25 +0200},
}

@Book{Crockett2014,
  author    = {Crockett, Louise H. and Elliot, Ross A. and Enderwitz, Martin A. and Stewart, Robert W.},
  date      = {2014},
  title     = {{The Zynq Book: Embedded Processing with the Arm Cortex-A9 on the Xilinx Zynq-7000 All Programmable Soc}},
  isbn      = {099297870X},
  location  = {Glasgow, GBR},
  publisher = {Strathclyde Academic Media},
  url       = {http://www.zynqbook.com/},
  abstract  = {This book is about the Zynq-7000 All Programmable System on Chip, the family of devices from Xilinx that combines an application-grade ARM Cortex-A9 processor with traditional FPGA logic fabric. Catering for both new and experienced readers, it covers fundamental issues in an accessible way, starting with a clear overview of the device architecture, and an introduction to the design tools and processes for developing a Zynq SoC. Later chapters progress to more advanced topics such as embedded systems development, IP block design and operating systems. Maintaining a 'real-world' perspective, the book also compares Zynq with other device alternatives, and considers end-user applications. The Zynq Book is accompanied by a set of practical tutorials hosted on a companion website. These tutorials will guide the reader through first steps with Zynq, following on to a complete, audio-based embedded systems design.},
  file      = {:D\:/74133/OneDrive - University of Edinburgh/Senior/Project/Vitis/The_Zynq_Book_ebook_chinese.pdf:PDF},
  groups    = {FPGA},
}

@Book{Davis2006,
  author    = {Davis, Timothy A.},
  date      = {2006},
  title     = {{Direct Methods for Sparse Linear Systems}},
  doi       = {10.1137/1.9780898718881},
  eprint    = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898718881},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://epubs.siam.org/doi/abs/10.1137/1.9780898718881},
  file      = {:Direct Methods for Sparse Linear Systems by Timothy A. Davis.pdf:PDF},
  groups    = {Algorithm},
}

@Article{Demmel1999,
  author       = {Demmel, James W. and Eisenstat, Stanley C. and Gilbert, John R. and Li, Xiaoye S. and Liu, Joseph W. H.},
  date         = {1999},
  journaltitle = {SIAM Journal on Matrix Analysis and Applications},
  title        = {{A Supernodal Approach to Sparse Partial Pivoting}},
  doi          = {10.1137/S0895479895291765},
  eprint       = {https://doi.org/10.1137/S0895479895291765},
  number       = {3},
  pages        = {720-755},
  url          = {https://doi.org/10.1137/S0895479895291765},
  volume       = {20},
  file         = {:A Supernodal Approach to Sparse Partial Pivoting.pdf:PDF},
  groups       = {Algorithm},
}

@Article{Boisvert1997,
  author = {Boisvert, Ronald and Pozo, Roldan and Remington, Karin},
  date   = {1997-03},
  title  = {{The Matrix Market Exchange Formats: Initial Design}},
  url    = {https://www.researchgate.net/publication/2630533_The_Matrix_Market_Exchange_Formats_Initial_Design},
  file   = {:The Matrix Market Exchange Formats - Initial Design.pdf:PDF},
  groups = {Algorithm},
}

@Article{Kastner2018,
  author       = {{Kastner}, Ryan and {Matai}, Janarbek and {Neuendorffer}, Stephen},
  date         = {2018-05},
  journaltitle = {arXiv e-prints},
  title        = {{Parallel Programming for FPGAs}},
  eid          = {arXiv:1805.03648},
  eprint       = {1805.03648},
  eprintclass  = {cs.AR},
  eprinttype   = {arXiv},
  pages        = {arXiv:1805.03648},
  adsnote      = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl       = {https://ui.adsabs.harvard.edu/abs/2018arXiv180503648K},
  file         = {:D\:/74133/OneDrive - University of Edinburgh/Senior/Project/Vitis/Parallel Programming for FPGAs.pdf:PDF},
  groups       = {FPGA},
  keywords     = {Computer Science - Hardware Architecture},
}

@Article{Amestoy2004,
  author       = {Amestoy, Patrick R. and Davis, Timothy A. and Duff, Iain S.},
  date         = {2004-09},
  journaltitle = {ACM Trans. Math. Softw.},
  title        = {{Algorithm 837: AMD, an Approximate Minimum Degree Ordering Algorithm}},
  doi          = {10.1145/1024074.1024081},
  issn         = {0098-3500},
  number       = {3},
  pages        = {381–388},
  url          = {https://doi.org/10.1145/1024074.1024081},
  volume       = {30},
  abstract     = {AMD is a set of routines that implements the approximate minimum degree ordering algorithm to permute sparse matrices prior to numerical factorization. There are versions written in both C and Fortran 77. A MATLAB interface is included.},
  file         = {:AMD, an approximate minimum degree ordering algorithm.pdf:PDF},
  groups       = {Algorithm},
  issue_date   = {September 2004},
  keywords     = {sparse matrices, Linear equations, minimum degree, ordering methods},
  location     = {New York, NY, USA},
  numpages     = {8},
  publisher    = {Association for Computing Machinery},
}

@Article{Nechma2015,
  author       = {Nechma, Tarek and Zwolinski, Mark},
  date         = {2015},
  journaltitle = {IEEE Transactions on Computers},
  title        = {{Parallel Sparse Matrix Solution for Circuit Simulation on FPGAs}},
  doi          = {10.1109/TC.2014.2308202},
  number       = {4},
  pages        = {1090-1103},
  volume       = {64},
  file         = {:Parallel_Sparse_Matrix_Solution_for_Circuit_Simulation_on_FPGAs.pdf:PDF},
  groups       = {FPGA},
}

@Article{Bollhoefer2019,
  author       = {Matthias Bollh{\"{o}}fer and Olaf Schenk and Radim Janal{\'{\i}}k and Steve Hamm and Kiran Gullapalli},
  date         = {2019},
  journaltitle = {CoRR},
  title        = {{State-of-The-Art Sparse Direct Solvers}},
  eprint       = {1907.05309},
  eprinttype   = {arxiv},
  volume       = {abs/1907.05309},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-05309.bib},
  file         = {:State-of-The-Art Sparse Direct Solvers.pdf:PDF},
  groups       = {Algorithm},
  timestamp    = {Sat, 23 Jan 2021 01:20:44 +0100},
}

@Article{Wang2004,
  author       = {Wang, Xiaofang and Ziavras, Sotirios G.},
  date         = {2004},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  title        = {{Parallel LU factorization of sparse matrices on FPGA-based configurable computing engines}},
  doi          = {https://doi.org/10.1002/cpe.748},
  eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.748},
  number       = {4},
  pages        = {319-343},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.748},
  volume       = {16},
  abstract     = {Abstract Configurable computing, where hardware resources are configured appropriately to match specific hardware designs, has recently demonstrated its ability to significantly improve performance for a wide range of computation-intensive applications. With steady advances in silicon technology, as predicted by Moore's Law, Field-Programmable Gate Array (FPGA) technologies have enabled the implementation of System-on-a-Programmable-Chip (SOPC or SOC) computing platforms, which, in turn, have given a significant boost to the field of configurable computing. It is possible to implement various specialized parallel machines in a single silicon chip. In this paper, we describe our design and implementation of a parallel machine on an SOPC development board, using multiple instances of a soft IP configurable processor; we use this machine for LU factorization. LU factorization is widely used in engineering and science to solve efficiently large systems of linear equations. Our implementation facilitates the efficient solution of linear equations at a cost much lower than that of supercomputers and networks of workstations. The intricacies of our FPGA-based design are presented along with tradeoff choices made for the purpose of illustration. Performance results prove the viability of our approach. Copyright © 2004 John Wiley \& Sons, Ltd.},
  file         = {:Parallel LU factorization of sparse matrices on FPGA‐based configurable.pdf:PDF},
  groups       = {FPGA},
  keywords     = {FPGA, LU factorization, matrix inversion, parallel processing, hardware design, SOPC/SOC},
}

@Article{Tarjan1972,
  author       = {Tarjan, Robert},
  date         = {1972},
  journaltitle = {SIAM Journal on Computing},
  title        = {{Depth-First Search and Linear Graph Algorithms}},
  doi          = {10.1137/0201010},
  eprint       = {https://doi.org/10.1137/0201010},
  number       = {2},
  pages        = {146-160},
  url          = {https://doi.org/10.1137/0201010},
  volume       = {1},
  file         = {:Depth-first search and linear graph algorithms.pdf:PDF},
  groups       = {Algorithm},
}

@InBook{Kapre2013,
  author    = {Kapre, Nachiket and DeHon, Andr{\'e}},
  booktitle = {High-Performance Computing Using FPGAs},
  date      = {2013},
  title     = {{Accelerating the SPICE Circuit Simulator Using an FPGA: A Case Study}},
  doi       = {10.1007/978-1-4614-1791-0_13},
  editor    = {Vanderbauwhede, Wim and Benkrid, Khaled},
  isbn      = {978-1-4614-1791-0},
  location  = {New York, NY},
  pages     = {389--427},
  publisher = {Springer New York},
  url       = {https://doi.org/10.1007/978-1-4614-1791-0_13},
  abstract  = {Spatial processing of sparse, irregular, double-precision floating-point computation using a single FPGA enables up to an order of magnitude speedup and energy-savings over a conventional microprocessor for the simulation program with integrated circuit emphasis (SPICE) circuit simulator. We develop a parallel, FPGA-based, heterogeneous architecture customized for accelerating the SPICE simulator to deliver this speedup. To properly parallelize the complete simulator, we decompose SPICE into its three constituent phases---Model Evaluation, Sparse Matrix-Solve, and Iteration Control---and customize a spatial architecture for each phase independently. Our heterogeneous FPGA organization mixes very large instruction word (VLIW), Dataflow and Streaming architectures into a cohesive, unified design. We program this parallel architecture with a high-level, domain-specific framework that identifies, exposes and exploits parallelism available in the SPICE circuit simulator using streaming (SCORE framework), data-parallel (Verilog-AMS models) and dataflow (KLU matrix solver) patterns. Our FPGA architecture is able to outperform conventional processors due to a combination of factors including high utilization of statically-scheduled resources, low-overhead dataflow scheduling of fine-grained tasks, and streaming, overlapped processing of the control algorithms. We expect approaches based on exploiting spatial parallelism to become important as frequency scaling continues to slow down and modern processing architectures turn to parallelism (e.g. multi-core, GPUs) due to constraints of power consumption.},
  file      = {:Accelerating the SPICE Circuit Simulator Using an FPGA - A Case Study.pdf:PDF},
  groups    = {FPGA},
}

@Article{Davis2004a,
  author       = {Davis, Timothy A.},
  date         = {2004-06},
  journaltitle = {ACM Trans. Math. Softw.},
  title        = {{Algorithm 832: UMFPACK V4.3---an Unsymmetric-Pattern Multifrontal Method}},
  doi          = {10.1145/992200.992206},
  issn         = {0098-3500},
  number       = {2},
  pages        = {196–199},
  url          = {https://doi.org/10.1145/992200.992206},
  volume       = {30},
  abstract     = {An ANSI C code for sparse LU factorization is presented that combines a column pre-ordering strategy with a right-looking unsymmetric-pattern multifrontal numerical factorization. The pre-ordering and symbolic analysis phase computes an upper bound on fill-in, work, and memory usage during the subsequent numerical factorization. User-callable routines are provided for ordering and analyzing a sparse matrix, computing the numerical factorization, solving a system with the LU factors, transposing and permuting a sparse matrix, and converting between sparse matrix representations. The simple user interface shields the user from the details of the complex sparse factorization data structures by returning simple handles to opaque objects. Additional user-callable routines are provided for printing and extracting the contents of these opaque objects. An even simpler way to use the package is through its MATLAB interface. UMFPACK is incorporated as a built-in operator in MATLAB 6.5 as x = Ab when A is sparse and unsymmetric.},
  file         = {:Algorithm 832 UMFPACK V4.3---an unsymmetric-pattern multifrontal method.pdf:PDF},
  groups       = {Algorithm},
  issue_date   = {June 2004},
  keywords     = {multifrontal method, linear equations, ordering methods, sparse nonsymmetric matrices},
  location     = {New York, NY, USA},
  numpages     = {4},
  publisher    = {Association for Computing Machinery},
}

@Article{Davis2004,
  author       = {Davis, Timothy A.},
  date         = {2004-06},
  journaltitle = {ACM Trans. Math. Softw.},
  title        = {{A Column Pre-Ordering Strategy for the Unsymmetric-Pattern Multifrontal Method}},
  doi          = {10.1145/992200.992205},
  issn         = {0098-3500},
  number       = {2},
  pages        = {165–195},
  url          = {https://doi.org/10.1145/992200.992205},
  volume       = {30},
  abstract     = {A new method for sparse LU factorization is presented that combines a column pre-ordering strategy with a right-looking unsymmetric-pattern multifrontal numerical factorization. The column ordering is selected to give a good a priori upper bound on fill-in and then refined during numerical factorization (while preserving the bound). Pivot rows are selected to maintain numerical stability and to preserve sparsity. The method analyzes the matrix and automatically selects one of three pre-ordering and pivoting strategies. The number of nonzeros in the LU factors computed by the method is typically less than or equal to those found by a wide range of unsymmetric sparse LU factorization methods, including left-looking methods and prior multifrontal methods.},
  file         = {:A column pre-ordering strategy for the unsymmetric-pattern multifrontal method.pdf:PDF},
  groups       = {Algorithm},
  issue_date   = {June 2004},
  keywords     = {ordering methods, sparse nonsymmetric matrices, linear equations, multifrontal method},
  location     = {New York, NY, USA},
  numpages     = {31},
  publisher    = {Association for Computing Machinery},
}

@Article{Li2005,
  author       = {Li, Xiaoye S.},
  date         = {2005-09},
  journaltitle = {ACM Trans. Math. Softw.},
  title        = {{An Overview of SuperLU: Algorithms, Implementation, and User Interface}},
  doi          = {10.1145/1089014.1089017},
  issn         = {0098-3500},
  number       = {3},
  pages        = {302–325},
  url          = {https://doi.org/10.1145/1089014.1089017},
  volume       = {31},
  abstract     = {We give an overview of the algorithms, design philosophy, and implementation techniques in the software SuperLU, for solving sparse unsymmetric linear systems. In particular, we highlight the differences between the sequential SuperLU (including its multithreaded extension) and parallel SuperLU_DIST. These include the numerical pivoting strategy, the ordering strategy for preserving sparsity, the ordering in which the updating tasks are performed, the numerical kernel, and the parallelization strategy. Because of the scalability concern, the parallel code is drastically different from the sequential one. We describe the user interfaces of the libraries, and illustrate how to use the libraries most efficiently depending on some matrix characteristics. Finally, we give some examples of how the solver has been used in large-scale scientific applications, and the performance.},
  groups       = {Algorithm},
  issue_date   = {September 2005},
  keywords     = {Sparse direct solver, parallelism, distributed-memory computers, scalability, supernodal factorization},
  location     = {New York, NY, USA},
  numpages     = {24},
  publisher    = {Association for Computing Machinery},
}

@Article{Kundert1986,
  author       = {Kundert, Kenneth S and others},
  date         = {1986},
  journaltitle = {Circuit Analysis, Simulation and Design},
  title        = {{Sparse matrix techniques}},
  number       = {pt 1},
  pages        = {160--169},
  volume       = {3},
  groups       = {Algorithm},
  publisher    = {North-Holland Amsterdam},
}

@Article{Duff1978,
  author       = {Duff, I. S. and Reid, J. K.},
  date         = {1978-06},
  journaltitle = {ACM Trans. Math. Softw.},
  title        = {{An Implementation of Tarjan's Algorithm for the Block Triangularization of a Matrix}},
  doi          = {10.1145/355780.355785},
  issn         = {0098-3500},
  number       = {2},
  pages        = {137–147},
  url          = {https://doi.org/10.1145/355780.355785},
  volume       = {4},
  groups       = {Algorithm},
  issue_date   = {June 1978},
  location     = {New York, NY, USA},
  numpages     = {11},
  publisher    = {Association for Computing Machinery},
}

@Article{Dongarra1997,
  author       = {Dongarra, Jack J. and Hammarling, Sven and Walker, David W.},
  date         = {1997-04},
  journaltitle = {Parallel Comput.},
  title        = {Key Concepts for Parallel Out-of-Core {LU} Factorization},
  doi          = {10.1016/S0167-8191(96)00096-8},
  issn         = {0167-8191},
  number       = {1–2},
  pages        = {49–70},
  url          = {https://doi.org/10.1016/S0167-8191(96)00096-8},
  volume       = {23},
  file         = {:Key Concepts for Parallel Out-of-Core LU Factorization.pdf:PDF},
  groups       = {Algorithm},
  issue_date   = {April 1997},
  keywords     = {out-of-core computation, LU factorization, parallel I/O, parallel computing},
  location     = {NLD},
  numpages     = {22},
  publisher    = {Elsevier Science Publishers B. V.},
}

@Article{Schenk2000,
  author       = {Schenk, O. and G{\"a}rtner, K. and Fichtner, W.},
  date         = {2000-03-01},
  journaltitle = {BIT Numerical Mathematics},
  title        = {{Efficient Sparse LU Factorization with Left-Right Looking Strategy on Shared Memory Multiprocessors}},
  doi          = {10.1023/A:1022326604210},
  issn         = {1572-9125},
  number       = {1},
  pages        = {158-176},
  url          = {https://doi.org/10.1023/A:1022326604210},
  volume       = {40},
  abstract     = {An efficient sparse LU factorization algorithm on popular shared memory multi-processors is presented. Pipelining parallelism is essential to achieve higher parallel efficiency and it is exploited with a left-right looking algorithm. No global barrier is used and a completely asynchronous scheduling scheme is one central point of the implementation. The algorithm has been successfully tested on SUN Enterprise, DEC AlphaServer, SGI Origin 2000 and Cray T90 and J90 parallel computers, delivering up to 2.3 GFlop/s on an eight processor DEC AlphaServer for medium-size semiconductor device simulations and structural engineering problems.},
  day          = {01},
  file         = {:Efficient Sparse LU Factorization with Left-Right Looking Strategy on Shared Memory Multiprocessors.pdf:PDF},
  groups       = {FPGA},
}

@PhdThesis{Kapre2010,
  author      = {Kapre, Nachiket Ganesh},
  date        = {2010-10-26},
  institution = {California Institute of Technology},
  title       = {{SPICE$^2$: A Spatial, Parallel Architecture for Accelerating the Spice Circuit Simulator}},
  location    = {Pasadena, California},
  type        = {phdthesis},
  file        = {:SPICE2 – A Spatial Parallel Architecture for Accelerating the SPICE Circuit Simulator.pdf:PDF},
  groups      = {SPICE},
}

@PhdThesis{Nagel1975,
  author      = {Nagel, Laurence W.},
  date        = {1975-05},
  institution = {EECS Department, University of California, Berkeley},
  title       = {{SPICE2: A Computer Program to Simulate Semiconductor Circuits}},
  url         = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/1975/9602.html},
  file        = {:SPICE2 - A Computer Program to Simulate Semiconductor Circuits.pdf:PDF},
  groups      = {SPICE},
  number      = {UCB/ERL M520},
}

@InProceedings{Merrett2011,
  author    = {Merrett, Michael and Asenov, Plamen and Wang, Yangang and Zwolinski, Mark and Reid, Dave and Millar, Campbell and Roy, Scott and Liu, Zhenyu and Furber, Steve and Asenov, Asen},
  booktitle = {2011 Design, Automation Test in Europe},
  date      = {2011},
  title     = {{Modelling circuit performance variations due to statistical variability: Monte Carlo static timing analysis}},
  doi       = {10.1109/DATE.2011.5763329},
  pages     = {1-4},
  file      = {:Modelling_circuit_performance_variations_due_to_statistical_variability_Monte_Carlo_static_timing_analysis.pdf:PDF},
  groups    = {SPICE},
}

@Article{Ho1975,
  author       = {Chung-Wen Ho and Ruehli, A. and Brennan, P.},
  date         = {1975},
  journaltitle = {IEEE Transactions on Circuits and Systems},
  title        = {{The modified nodal approach to network analysis}},
  doi          = {10.1109/TCS.1975.1084079},
  number       = {6},
  pages        = {504-509},
  volume       = {22},
  file         = {:The_modified_nodal_approach_to_network_analysis.pdf:PDF},
  groups       = {SPICE},
}

@Article{Duff1989,
  author       = {Duff, I. S. and Grimes, Roger G. and Lewis, John G.},
  date         = {1989-03},
  journaltitle = {ACM Trans. Math. Softw.},
  title        = {{Sparse Matrix Test Problems}},
  doi          = {10.1145/62038.62043},
  issn         = {0098-3500},
  number       = {1},
  pages        = {1–14},
  url          = {https://doi.org/10.1145/62038.62043},
  volume       = {15},
  abstract     = {We describe the Harwell-Boeing sparse matrix collection, a set of standard test matrices for sparse matrix problems. Our test set comprises problems in linear systems, least squares, and eigenvalue calculations from a wide variety of scientific and engineering disciplines. The problems range from small matrices, used as counter-examples to hypotheses in sparse matrix research, to large test cases arising in large-scale computation. We offer the collection to other researchers as a standard benchmark for comparative studies of algorithms. The procedures for obtaining and using the test collection are discussed. We also describe the guidelines for contributing further test problems to the collection.},
  file         = {:Sparse Matrix Test Problems.pdf:PDF},
  groups       = {Algorithm},
  issue_date   = {March 1989},
  location     = {New York, NY, USA},
  numpages     = {14},
  publisher    = {Association for Computing Machinery},
}

@Article{Eberly2006,
  author       = {Wayne Eberly and Mark Giesbrecht and Pascal Giorgi and Arne Storjohann and Gilles Villard},
  date         = {2006},
  journaltitle = {CoRR},
  title        = {{Solving Sparse Integer Linear Systems}},
  eprint       = {cs/0603082},
  eprinttype   = {arXiv},
  url          = {http://arxiv.org/abs/cs/0603082},
  volume       = {abs/cs/0603082},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/abs-cs-0603082.bib},
  file         = {:Solving Sparse Integer Linear Systems.pdf:PDF},
  groups       = {Algorithm},
  timestamp    = {Mon, 13 Aug 2018 16:46:22 +0200},
}

@Article{Buell2007,
  author       = {D. Buell and K. Gaj and T. El-Ghazawi and V. Kindratenko},
  date         = {2007-03},
  journaltitle = {Computer},
  title        = {{Guest Editors' Introduction: High-Performance Reconfigurable Computing}},
  doi          = {10.1109/MC.2007.91},
  issn         = {1558-0814},
  number       = {03},
  pages        = {23-27},
  volume       = {40},
  file         = {:Guest Editors' Introduction High-Performance Reconfigurable Computing.pdf:PDF},
  groups       = {FPGA},
  keywords     = {reconfigurable computing;fpgas;hprcs},
  location     = {Los Alamitos, CA, USA},
  publisher    = {IEEE Computer Society},
}

@Article{Kuon2007,
  author       = {Kuon, Ian and Rose, Jonathan},
  date         = {2007},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  title        = {{Measuring the Gap Between FPGAs and ASICs}},
  doi          = {10.1109/TCAD.2006.884574},
  number       = {2},
  pages        = {203-215},
  volume       = {26},
  file         = {:Measuring_the_Gap_Between_FPGAs_and_ASICs.pdf:PDF},
  groups       = {FPGA},
}

@InBook{Koch2016,
  author    = {Koch, Dirk and Ziener, Daniel and Hannig, Frank},
  booktitle = {FPGAs for Software Programmers},
  date      = {2016},
  title     = {{FPGA Versus Software Programming: Why, When, and How?}},
  doi       = {10.1007/978-3-319-26408-0_1},
  editor    = {Koch, Dirk and Hannig, Frank and Ziener, Daniel},
  isbn      = {978-3-319-26408-0},
  location  = {Cham},
  pages     = {1--21},
  publisher = {Springer International Publishing},
  url       = {https://doi.org/10.1007/978-3-319-26408-0_1},
  abstract  = {This chapter provides background information for readers who are interested in the philosophy and technology behind FPGAs. We present this from a software engineer's viewpoint without hiding the hardware specific characteristics of FPGAs. The chapter discusses the architecture and programming models as well as the pros and cons of CPUs, GPUs and FPGAs. The operation of FPGAs will be described as well as the major steps that are needed to map a circuit description on an FPGA. This will provide a deep enough understanding of the characteristics of an FPGA and how this helps in accelerating certain parts of an application.},
  file      = {:2016_Book_FPGAsForSoftwareProgrammers.pdf:PDF},
  groups    = {FPGA},
}

@PhdThesis{AlAghbari2018,
  author      = {Al-Aghbari, Amran},
  date        = {2018-12},
  institution = {King Fahd University of Petroleum and Minerals},
  title       = {{Cloud-Based FPGA Custom Computing Machines}},
  doi         = {10.13140/RG.2.2.35939.37926},
  file        = {:Amran_PhD_Thesis_Cloud-basedFCCMs.pdf:PDF},
  groups      = {FPGA},
}

@Article{Alappat2020,
  author       = {Alappat, Christie and Basermann, Achim and Bishop, Alan R. and Fehske, Holger and Hager, Georg and Schenk, Olaf and Thies, Jonas and Wellein, Gerhard},
  date         = {2020-06},
  journaltitle = {ACM Trans. Parallel Comput.},
  title        = {{A Recursive Algebraic Coloring Technique for Hardware-Efficient Symmetric Sparse Matrix-Vector Multiplication}},
  doi          = {10.1145/3399732},
  issn         = {2329-4949},
  number       = {3},
  url          = {https://doi.org/10.1145/3399732},
  volume       = {7},
  abstract     = {The symmetric sparse matrix-vector multiplication (SymmSpMV) is an important building block for many numerical linear algebra kernel operations or graph traversal applications. Parallelizing SymmSpMV on today’s multicore platforms with up to 100 cores is difficult due to the need to manage conflicting updates on the result vector. Coloring approaches can be used to solve this problem without data duplication, but existing coloring algorithms do not take load balancing and deep memory hierarchies into account, hampering scalability and full-chip performance. In this work, we propose the recursive algebraic coloring engine (RACE), a novel coloring algorithm and open-source library implementation that eliminates the shortcomings of previous coloring methods in terms of hardware efficiency and parallelization overhead. We describe the level construction, distance-k coloring, and load balancing steps in RACE, use it to parallelize SymmSpMV, and compare its performance on 31 sparse matrices with other state-of-the-art coloring techniques and Intel MKL on two modern multicore processors. RACE outperforms all other approaches substantially. By means of a parameterized roofline model, we analyze the SymmSpMV performance in detail and discuss outliers. While we focus on SymmSpMV in this article, our algorithm and software are applicable to any sparse matrix operation with data dependencies that can be resolved by distance-k coloring.},
  articleno    = {19},
  comment      = {pardiso},
  groups       = {Algorithm},
  issue_date   = {September 2020},
  keywords     = {sparse symmetric matrix-vector multiplication, Sparse matrix, graph algorithms, memory hierarchies, scheduling, graph coloring},
  location     = {New York, NY, USA},
  numpages     = {37},
  publisher    = {Association for Computing Machinery},
}

@Article{Li2003,
  author       = {Li, Xiaoye S. and Demmel, James W.},
  date         = {2003-06},
  journaltitle = {ACM Trans. Math. Softw.},
  title        = {{SuperLU\textunderscore DIST: A Scalable Distributed-Memory Sparse Direct Solver for Unsymmetric Linear Systems}},
  doi          = {10.1145/779359.779361},
  issn         = {0098-3500},
  number       = {2},
  pages        = {110–140},
  url          = {https://doi.org/10.1145/779359.779361},
  volume       = {29},
  abstract     = {We present the main algorithmic features in the software package SuperLU_DIST, a distributed-memory sparse direct solver for large sets of linear equations. We give in detail our parallelization strategies, with a focus on scalability issues, and demonstrate the software's parallel performance and scalability on current machines. The solver is based on sparse Gaussian elimination, with an innovative static pivoting strategy proposed earlier by the authors. The main advantage of static pivoting over classical partial pivoting is that it permits a priori determination of data structures and communication patterns, which lets us exploit techniques used in parallel sparse Cholesky algorithms to better parallelize both LU decomposition and triangular solution on large-scale distributed machines.},
  address      = {New York, NY, USA},
  groups       = {Algorithm},
  issue_date   = {June 2003},
  journal      = {ACM Trans. Math. Softw.},
  keywords     = {Sparse direct solver, scalability, supernodal factorization, distributed-memory computers, parallelism},
  location     = {New York, NY, USA},
  numpages     = {31},
  publisher    = {Association for Computing Machinery},
}

@TechReport{Demmel1997,
  author      = {Demmel, James W. and Gilbert, John R. and Li, Xiaoye S.},
  date        = {1997-02},
  institution = {EECS Department, University of California, Berkeley},
  title       = {{An Asynchronous Parallel Supernodal Algorithm for Sparse Gaussian Elimination}},
  number      = {UCB/CSD-97-943},
  url         = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/1997/5622.html},
  comment     = {SuperLU_MT},
  groups      = {Algorithm},
}

@TechReport{Rothberg1991,
  author    = {Rothberg, Edward and Gupta, Anoop},
  date      = {1991},
  title     = {{An Evaluation of Left-Lookikng, Right-Looking and Multifrontal Approaches to Sparse Cholesky Factorization on Hierarchical Memory Machines}},
  location  = {Stanford, CA, USA},
  abstract  = {In this paper we present a comprehensive analysis of the performance of a variety of sparse Cholesky factorization methods on hierarchical-memory machines. We investigate methods that vary along two different axes. Along the first axis, we consider three different high-level approaches to sparse factorization: left-looking, right-looking, and multifrontal. Along the second axis, we consider the implementation of each of these high-level approaches using different sets of primitives. The primitives vary based on the structures they manipulate. One important structure in sparse Cholesky factorization is a single column of the matrix. We first consider primitives that manipulate single columns. These are the most commonly used primitives for expressing the sparse Cholesky computation. Another important structure is the supernode, a set of columns with identical non-zero structures. We consider sets of primitives that exploit the supemodal structure of the matrix to varying degrees. We find that primitives that manipulate larger structures greatly increase the amount of exploitable data reuse, thus leading to dramatically higher perfommance on hierarchical-memory machines. We observe performance increases of two to three times when comparing methods based on primitives that make extensive use of the supernodal structure to methods based on primitives that manipulate columns. We also find that the overall approach (left-looking, right-looking, or multifrontal) is less important for performance than the particular set of primitives used to implement the approach.},
  file      = {:An Evaluation of Left-Looking, Right-Looking and Multifrontal.pdf:PDF},
  groups    = {Algorithm},
  publisher = {Stanford University},
}

@InProceedings{Zhuo2006,
  author    = {Zhuo, Ling and Prasanna, Viktor K.},
  booktitle = {2006 International Conference on Field Programmable Logic and Applications},
  title     = {{High-Performance and Parameterized Matrix Factorization on FPGAs}},
  doi       = {10.1109/FPL.2006.311238},
  pages     = {1-6},
  groups    = {FPGA},
  year      = {2006},
}

@TechReport{Xilinx2021,
  author      = {Xilinx},
  date        = {2021-10-22},
  institution = {Xilinx, Inc},
  title       = {{Vitis Unified Software Platform Documentation: Application Acceleration Development}},
  url         = {https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration},
  version     = {2021.2},
  groups      = {FPGA},
}

@TechReport{Xilinx2021a,
  author      = {Xilinx},
  date        = {2021-09-24},
  institution = {Xilinx, Inc},
  title       = {{Alveo U200 and U250 Data CenterAccelerator Cards Data Sheet}},
  url         = {https://www.xilinx.com/products/boards-and-kits/alveo/u250.html#documentation},
  version     = {1.4},
  groups      = {FPGA},
}

@Article{Pothen1990,
  author       = {Pothen, Alex and Fan, Chin-Ju},
  date         = {1990-12},
  journaltitle = {ACM Trans. Math. Softw.},
  title        = {{Computing the Block Triangular Form of a Sparse Matrix}},
  doi          = {10.1145/98267.98287},
  issn         = {0098-3500},
  number       = {4},
  pages        = {303–324},
  volume       = {16},
  abstract     = {We consider the problem of permuting the rows and columns of a rectangular or square, unsymmetric sparse matrix to compute its block triangular form. This block triangular form is based on a canonical decomposition of bipartite graphs induced by a maximum matching and was discovered by Dulmage and Mendelsohn. We describe implementations of algorithms to compute the block triangular form and provide computational results on sparse matrices from test collections. Several applications of the block triangular form are also included.},
  groups       = {Algorithm},
  issue_date   = {Dec. 1990},
  location     = {New York, NY, USA},
  numpages     = {22},
  publisher    = {Association for Computing Machinery},
}

@Article{Sargent1964,
  author  = {Sargent, Roger and Westerberg, Arthur},
  title   = {{SPEED-UP in Chemical Engineering Design}},
  doi     = {10.13140/2.1.2585.6001},
  pages   = {190},
  volume  = {42},
  groups  = {Algorithm},
  journal = {Transactions of the Institution of Chemical Engineers},
  month   = {01},
  year    = {1964},
}

@InBook{Rose1975,
  author    = {Rose, Donald J. and Tarjan, R. Endre},
  booktitle = {Proceedings of the Seventh Annual ACM Symposium on Theory of Computing},
  date      = {1975},
  title     = {{Algorithmic Aspects of Vertex Elimination}},
  doi       = {10.1145/800116.803775},
  isbn      = {9781450374194},
  location  = {New York, NY, USA},
  pages     = {245–254},
  publisher = {Association for Computing Machinery},
  abstract  = {We consider a graph-theoretic elimination process which is related to performing Gaussian elimination on sparse symmetric and unsymmetric systems of linear equations. We discuss good algorithms for finding elimination orderings, showing that a generalization of breadth-first search, called lexicographic search, can be used to find perfect orderings in 0(n+e) time and minimal orderings in 0(ne) time, if the problem graph is undirected and has n vertices and e edges. We also give efficient (though slower) algorithms for generating such orderings on directed graphs. We claim that the minimum ordering problem for directed graphs is NP-complete, and conjecture that it is also NP-complete for undirected graphs. We include a brief discussion of the relation of elimination to transitive closure and discuss some unresolved, more general, issues.},
  groups    = {Algorithm},
  numpages  = {10},
}

@Article{Yannakakis1981,
  author       = {Yannakakis, Mihalis},
  date         = {1981},
  journaltitle = {SIAM Journal on Algebraic Discrete Methods},
  title        = {{Computing the Minimum Fill-In is NP-Complete}},
  doi          = {10.1137/0602010},
  eprint       = {https://doi.org/10.1137/0602010},
  number       = {1},
  pages        = {77-79},
  url          = {https://doi.org/10.1137/0602010},
  volume       = {2},
  abstract     = {We show that the following problem is NP-complete. Given a graph, find the minimum number of edges (fill-in) whose addition makes the graph chordal. This problem arises in the solution of sparse symmetric positive definite systems of linear equations by Gaussian elimination.},
  groups       = {Algorithm},
}

@Article{George1989,
  author       = {George, Alan and Liu, Joseph W.H.},
  date         = {1989},
  journaltitle = {SIAM Review},
  title        = {{The Evolution of the Minimum Degree Ordering Algorithm}},
  doi          = {10.1137/1031001},
  eprint       = {https://doi.org/10.1137/1031001},
  number       = {1},
  pages        = {1-19},
  url          = {https://doi.org/10.1137/1031001},
  volume       = {31},
  abstract     = {Over the past fifteen years, the implementation of the minimum degree algorithm has received much study, and many important enhancements have been made to it. This paper describes these various enhancements, their historical development, and some experiments showing how very effective they are in improving the execution time of the algorithm. A shortcoming is also presented that exists in all of the widely used implementations of the algorithm, namely, that the quality of the ordering provided by the implementations is surprisingly sensitive to the initial ordering. For example, changing the input ordering can lead to an increase (or decrease) of as much as a factor of three in the cost of the subsequent numerical factorization. This sensitivity is caused by the lack of an effective tie-breaking strategy, and the authors’ experiments illustrate the importance of developing such a strategy},
  groups       = {Algorithm},
}

@TechReport{Intel2021,
  author      = {Intel},
  date        = {2021-06-21},
  institution = {Intel Corporation},
  title       = {{Intel\textsuperscript{\textregistered} C++ Compiler Classic Developer Guide and Reference}},
  version     = {2021.5},
  file        = {:cpp_compiler_classic.pdf:PDF},
  groups      = {Algorithm},
}

@Article{Kalomiros2019,
  author       = {Kalomiros, John and Nabi, Syed Waqar and Vanderbauwhede, Wim},
  date         = {2019},
  journaltitle = {International Journal of Reconfigurable Computing},
  title        = {{Automatic Pipelining and Vectorization of Scientific Code for FPGAs}},
  doi          = {10.1155/2019/7348013},
  issn         = {1687-7195},
  pages        = {7348013},
  volume       = {2019},
  file         = {:Automatic Pipelining and Vectorization of Scientific Code for FPGAs.pdf:PDF},
  groups       = {FPGA},
  publisher    = {Hindawi},
}

@Misc{Xilinx2018,
  author       = {Xilinx},
  date         = {2018-09-06},
  title        = {{Alveo U250 Data Center Accelerator Card Active Option}},
  organization = {Xilinx, Inc},
  url          = {https://www.xilinx.com/products/boards-and-kits/alveo/u250.html#specifications},
  urldate      = {2022-04-26},
  groups       = {Algorithm},
}

@Article{Li2020,
  author       = {Ruoshi Li and Hongjing Huang and Zeke Wang and Zhiyuan Shao and Xiaofei Liao and Hai Jin},
  date         = {2020},
  journaltitle = {CoRR},
  title        = {{Optimizing Memory Performance of Xilinx FPGAs under Vitis}},
  eprint       = {2010.08916},
  eprinttype   = {arXiv},
  url          = {https://arxiv.org/abs/2010.08916},
  volume       = {abs/2010.08916},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-08916.bib},
  groups       = {FPGA},
  timestamp    = {Wed, 21 Oct 2020 12:11:48 +0200},
}

@Article{Peng2020,
  author       = {Richard Peng and Santosh S. Vempala},
  date         = {2020},
  journaltitle = {CoRR},
  title        = {{Solving Sparse Linear Systems Faster than Matrix Multiplication}},
  eprint       = {2007.10254},
  eprinttype   = {arXiv},
  url          = {https://arxiv.org/abs/2007.10254},
  volume       = {abs/2007.10254},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2007-10254.bib},
  groups       = {Algorithm},
  timestamp    = {Tue, 28 Jul 2020 14:46:12 +0200},
}

@Article{Davis2011,
  author       = {Davis, Timothy A. and Hu, Yifan},
  date         = {2011-12},
  journaltitle = {ACM Trans. Math. Softw.},
  title        = {{The University of Florida Sparse Matrix Collection}},
  doi          = {10.1145/2049662.2049663},
  issn         = {0098-3500},
  number       = {1},
  volume       = {38},
  abstract     = {We describe the University of Florida Sparse Matrix Collection, a large and actively growing set of sparse matrices that arise in real applications. The Collection is widely used by the numerical linear algebra community for the development and performance evaluation of sparse matrix algorithms. It allows for robust and repeatable experiments: robust because performance results with artificially generated matrices can be misleading, and repeatable because matrices are curated and made publicly available in many formats. Its matrices cover a wide spectrum of domains, include those arising from problems with underlying 2D or 3D geometry (as structural engineering, computational fluid dynamics, model reduction, electromagnetics, semiconductor devices, thermodynamics, materials, acoustics, computer graphics/vision, robotics/kinematics, and other discretizations) and those that typically do not have such geometry (optimization, circuit simulation, economic and financial modeling, theoretical and quantum chemistry, chemical process simulation, mathematics and statistics, power networks, and other networks and graphs). We provide software for accessing and managing the Collection, from MATLAB™, Mathematica™, Fortran, and C, as well as an online search capability. Graph visualization of the matrices is provided, and a new multilevel coarsening scheme is proposed to facilitate this task.},
  articleno    = {1},
  groups       = {SPICE},
  issue_date   = {November 2011},
  keywords     = {Graph drawing, performance evaluation, sparse matrices, multilevel algorithms},
  location     = {New York, NY, USA},
  numpages     = {25},
  publisher    = {Association for Computing Machinery},
}

@Article{Spielman2008,
  author       = {Daniel A. Spielman and Shang{-}Hua Teng},
  date         = {2008},
  journaltitle = {CoRR},
  title        = {{Spectral Sparsification of Graphs}},
  eprint       = {0808.4134},
  eprinttype   = {arXiv},
  url          = {http://arxiv.org/abs/0808.4134},
  volume       = {abs/0808.4134},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/abs-0808-4134.bib},
  groups       = {Algorithm},
  timestamp    = {Mon, 13 Aug 2018 16:47:00 +0200},
}

@Article{Koutis2012,
  author       = {Koutis, Ioannis and Miller, Gary L. and Peng, Richard},
  date         = {2012-10},
  journaltitle = {Commun. ACM},
  title        = {{A Fast Solver for a Class of Linear Systems}},
  doi          = {10.1145/2347736.2347759},
  issn         = {0001-0782},
  number       = {10},
  pages        = {99–107},
  volume       = {55},
  abstract     = {The solution of linear systems is a problem of fundamental theoretical importance but also one with a myriad of applications in numerical mathematics, engineering, and science. Linear systems that are generated by real-world applications frequently fall into special classes. Recent research led to a fast algorithm for solving symmetric diagonally dominant (SDD) linear systems. We give an overview of this solver and survey the underlying notions and tools from algebra, probability, and graph algorithms. We also discuss some of the many and diverse applications of SDD solvers.},
  groups       = {Algorithm},
  issue_date   = {October 2012},
  location     = {New York, NY, USA},
  numpages     = {9},
  publisher    = {Association for Computing Machinery},
}

@Conference{Kwack2016,
  author   = {J Kwack and G Bauer and Seid Koric},
  date     = {2016},
  title    = {{Performance test of parallel linear equation solvers on Blue Waters--Cray XE6/XK7 system}},
  language = {English (US)},
  abstract = {Parallel linear equation solvers are one of the most important components determining the scalability and efficiency of many supercomputing applications. Several groups and companies are leading the development of linear system solver libraries for HPC applications. In this paper, we present an objective performance test study for the solvers available on a Cray XE6/XK7 supercomputer, named Blue Waters, at National Center for Supercomputing Applications (NCSA). A series of non-symmetric matrices are created through mesh refinements of a CFD problem. PETSc, MUMPS, SuperLU, Cray LibSci, Intel PARDISO, IBM WSMP, ACML, GSL, NVIDIA cuSOLVER and AmgX solver are employed for the performance test. CPU-compatible libraries are tested on XE6 nodes while GPU-compatible libraries are tested on XK7 nodes. We present scalability test results of each library on Blue Waters, and how far and fast the employed libraries can solve the series of matrices.},
  groups   = {Algorithm},
  keywords = {parallel linear equation solver, dense direct solver, sparse direct solver, sparse iterative solver, CPU-compatible library, GPU-compatible library},
  year     = {2016},
}

@Article{Lant2020,
  author       = {Lant, Joshua and Navaridas, Javier and Luján, Mikel and Goodacre, John},
  date         = {2020},
  journaltitle = {IEEE Micro},
  title        = {{Toward FPGA-Based HPC: Advancing Interconnect Technologies}},
  doi          = {10.1109/MM.2019.2950655},
  number       = {1},
  pages        = {25-34},
  volume       = {40},
  file         = {:Toward_FPGA-Based_HPC_Advancing_Interconnect_Technologies.pdf:PDF},
  groups       = {Impact},
}

@Article{Pundir2021,
  author       = {Nitin Pundir and Fahim Rahman and Farimah Farahmandi and Mark Mohammad Tehranipoor},
  date         = {2021},
  journaltitle = {IACR Cryptol. ePrint Arch.},
  title        = {{What is All the FaaS About? - Remote Exploitation of FPGA-as-a-Service Platforms}},
  pages        = {746},
  volume       = {2021},
  file         = {:What is All the FaaS About - Remote Exploitation of FPGA-as-a-Service Platforms.pdf:PDF},
  groups       = {Impact},
}

@Online{CUM2020,
  author       = {{Close-Up Media, Inc.}},
  date         = {2020-05-14},
  title        = {{Research and Markets Adds Report: Field Programmable Gate Array Market}},
  url          = {https://link-gale-com.ezproxy.is.ed.ac.uk/apps/doc/A623769864/ITOF?u=ed_itw&sid=bookmark-ITOF&xid=24ccb019},
  language     = {English},
  urldate      = {2022-04-29},
  database     = {Gale General OneFile},
  groups       = {Impact},
  journaltitle = {Wireless News},
  keywords     = {Data centers, Programmable logic arrays, Data centres},
  number       = {NA},
  pages        = {NA},
}

@Article{Xu2009,
  author       = {Xu, Ning-Yi and Cai, Xiong-Fei and Gao, Rui and Zhang, Lei and Hsu, Feng-Hsiung},
  date         = {2009-01},
  journaltitle = {ACM Trans. Reconfigurable Technol. Syst.},
  title        = {{FPGA Acceleration of RankBoost in Web Search Engines}},
  doi          = {10.1145/1462586.1462588},
  issn         = {1936-7406},
  number       = {4},
  volume       = {1},
  abstract     = {Search relevance is a key measurement for the usefulness of search engines. Shift of search relevance among search engines can easily change a search company's market cap by tens of billions of dollars. With the ever-increasing scale of the Web, machine learning technologies have become important tools to improve search relevance ranking. RankBoost is a promising algorithm in this area, but it is not widely used due to its long training time. To reduce the computation time for RankBoost, we designed a FPGA-based accelerator system and its upgraded version. The accelerator, plugged into a commodity PC, increased the training speed on MSN search engine data up to 1800x compared to the original software implementation on a server. The proposed accelerator has been successfully used by researchers in the search relevance ranking.},
  address      = {New York, NY, USA},
  articleno    = {19},
  groups       = {Impact},
  issue_date   = {January 2009},
  journal      = {ACM Trans. Reconfigurable Technol. Syst.},
  keywords     = {hardware acceleration, FPGA},
  location     = {New York, NY, USA},
  numpages     = {19},
  publisher    = {Association for Computing Machinery},
}

@Article{Wang2017,
  author       = {Wang, Chao and Gong, Lei and Yu, Qi and Li, Xi and Xie, Yuan and Zhou, Xuehai},
  date         = {2017},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  title        = {{DLAU: A Scalable Deep Learning Accelerator Unit on FPGA}},
  doi          = {10.1109/TCAD.2016.2587683},
  number       = {3},
  pages        = {513-517},
  volume       = {36},
  groups       = {Impact},
}

@Report{Xilinx2020,
  author      = {Xilinx},
  date        = {2020-11-02},
  institution = {Xilinx, Inc},
  title       = {{Fiscal Year 2020 Xilinx Corporate Responsibility Report}},
  url         = {https://www.xilinx.com/about/community-engagement.html},
  file        = {:FY20 Xilinx Corporate Responsibility Report.pdf:PDF},
  groups      = {Impact},
}

@Report{Subramaniyam2015,
  author      = {Karthikeyan Subramaniyam},
  date        = {2015-10-15},
  institution = {Xilinx, Inc},
  title       = {{Proven Power Reduction with Xilinx UltraScale FPGAs}},
  type        = {techreport},
  number      = {WP466},
  version     = {1.1},
  file        = {:wp466-proven-ultrascale-power-leaders.pdf:PDF},
  groups      = {Impact},
}

@Article{Etro2009,
  author       = {Federico Etro},
  date         = {2009},
  journaltitle = {Review of Business and Economic Literature},
  title        = {{The Economic Impact of Cloud Computing on Business Creation, Employment and Output in Europe. An application of the Endogenous Market Structures Approach to a GPT innovation}},
  number       = {2},
  pages        = {179-208},
  url          = {https://ideas.repec.org/a/ete/revbec/20090204.html},
  volume       = {0},
  abstract     = {Cloud computing is a new general purpose Internet-based technology through which information is stored in servers and provided as a service and on-demand to clients. Adopting the endogenous market structures approach to macroeconomics, we analyze the economic impact of the gradual introduction of cloud computing and we emphasize its role in fostering business creation and competition thanks to the reduction of the fixed costs of entry in ICT capital. Our calculations based on a DSGE model show a significative impact for the European Union with the creation of a few hundred thousands new SMEs and a significant contribution to growth. Governments could enhance these benefits by subsidizing the adoption of cloud computing solutions.},
  groups       = {Impact},
  keywords     = {Endogenous market structure; Cloud computing; Utility computing; General purpose technology; Firms’},
}

@Article{V2019GREENCC,
  author       = {Bindhu V and Vijesh joe S},
  date         = {2019-09},
  journaltitle = {Journal of IoT in Social, Mobile, Analytics, and Cloud},
  title        = {GREEN CLOUD COMPUTING SOLUTION FOR OPERATIONAL COST EFFICIENCY AND ENVIRONMENTAL IMPACT REDUCTION},
  number       = {2},
  pages        = {120-128},
  volume       = {1},
  file         = {:GREEN_CLOUD_COMPUTING_SOLUTION-IRO-Journals-1_2_5.pdf:PDF},
  groups       = {Impact},
  journal      = {Journal of ISMAC},
}

@Article{Patil2020,
  author       = {Patil, Archana and Rekha, Patil},
  date         = {2020-07},
  journaltitle = {International Conference on Sustainable Computing in Science, Technology \& Management},
  title        = {{An Analysis Report on Green Cloud Current Trends and Future Research Challenges}},
  doi          = {10.31221/osf.io/hymr7},
  url          = {arabixiv.org/hymr7},
  groups       = {Impact},
  publisher    = {Arabixiv},
}

@Online{Alsop2022,
  author       = {Thomas Alsop},
  date         = {2022-04-06},
  title        = {{Semiconductor integrated circuits global revenue 2009-2022}},
  url          = {https://www.statista.com/statistics/519456/forecast-of-worldwide-semiconductor-sales-of-integrated-circuits/},
  organization = {Statista},
  urldate      = {2022-04-29},
  groups       = {Impact},
}

@InProceedings{Wu2011,
  author    = {Wu, Wei and Shan, Yi and Chen, Xiaoming and Wang, Yu and Yang, Huazhong},
  booktitle = {Reconfigurable Computing: Architectures, Tools and Applications},
  date      = {2011},
  title     = {{FPGA Accelerated Parallel Sparse Matrix Factorization for Circuit Simulations}},
  doi       = {10.1007/978-3-642-19475-7_33},
  editor    = {Koch, Andreas and Krishnamurthy, Ram and McAllister, John and Woods, Roger and El-Ghazawi, Tarek},
  isbn      = {978-3-642-19475-7},
  location  = {Berlin, Heidelberg},
  pages     = {302--315},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Sparse matrix factorization is a critical step for the circuit simulation problem, since it is time consuming and computed repeatedly in the flow of circuit simulation. To accelerate the factorization of sparse matrices, a parallel CPU+FPGA based architecture is proposed in this paper. While the pre-processing of the matrix is implemented on CPU, the parallelism of numeric factorization is explored by processing several columns of the sparse matrix simultaneously on a set of processing elements (PE) in FPGA. To cater for the requirements of circuit simulation, we also modified the Gilbert/Peierls (G/P) algorithm and considered the scalability of our architecture. Experimental results on circuit matrices from the University of Florida Sparse Matrix Collection show that our architecture achieves speedup of 0.5x-5.36x compared with the CPU KLU results.},
  file      = {:FPGA Accelerated Parallel Sparse Matrix Factorization for Circuit Simulations.pdf:PDF},
  groups    = {FPGA},
}

@InProceedings{Mocanu2013,
  author    = {Mocanu, Adrian and Ţăpus, Nicolae},
  booktitle = {2013 IEEE 9th International Conference on Intelligent Computer Communication and Processing (ICCP)},
  date      = {2013},
  title     = {Sparse matrix permutations to a block triangular form in a distributed environment},
  doi       = {10.1109/ICCP.2013.6646131},
  pages     = {331-338},
  groups    = {Algorithm},
}

@Online{DigitalisationWorld2021,
  author  = {{Digitalisation World}},
  date    = {2021-07-22},
  title   = {The growing market for High-Performance Computing (HPC)},
  url     = {The growing market for High-Performance Computing (HPC)},
  urldate = {2022-05-01},
  groups  = {Impact},
}

@Comment{jabref-meta: databaseType:biblatex;}

@Comment{jabref-meta: fileDirectoryLatex-74133-YCZhang:D:\\74133\\OneDrive - University of Edinburgh\\Senior\\Project\\thesis;}

@Comment{jabref-meta: fileDirectoryLatex-ethan-uoe:/home/ethan/thesis;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:FPGA\;0\;1\;0x0000ffff\;\;\;;
1 StaticGroup:Algorithm\;0\;1\;0xffff00ff\;\;\;;
1 StaticGroup:GPU\;0\;0\;0xff0000ff\;\;\;;
1 StaticGroup:SPICE\;0\;1\;0xff00ffff\;\;Other circuit simulation\;;
1 StaticGroup:Impact\;0\;1\;0xffff99ff\;\;\;;
}
